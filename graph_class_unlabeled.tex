\input{/Users/jovo/Research/papers/meta/latex_paper.tex} 
\usepackage{url}
\lhead{Vogelstein JT, et al}
\rhead{short title}

\newcommand{\mcE}{\mathcal{E}}
\newcommand{\mcT}{\mathcal{T}}
\newcommand{\mcG}{\mathcal{G}}
\newcommand{\mcM}{\mathcal{M}}
\newcommand{\mcL}{\mathcal{L}}
\newcommand{\hatmcE}{\widehat{\mcE}}
\newcommand{\hatp}{\widehat{p}}
\newcommand{\hatP}{\widehat{P}}
\newcommand{\hatQ}{\widehat{Q}}
\newcommand{\hatL}{\widehat{L}}
\newcommand{\mhP}{\widehat{\PP}}
\newcommand{\tildeA}{\widetilde{A}}

\newcommand{\defa}{\begin{defi}}
\newcommand{\defb}{\end{defi}}

\newtheorem{Rem}{Remark}%[section]
\newtheorem{Alg}{Algorithm}%[section]
\newtheorem{Thm}{Theorem}[section]
%\theoremstyle{marginbreak}
\newtheorem{Lem}{Lemma}[section]
%\newtheorem{Lem}[Cor]{Lemma}
%\theoremstyle{change}
%\theorembodyfont{\itshape}
\newtheorem{Def}{Definition}[section]

\title{PRE-DRAFT: On unlabeled graph classification}

\author{cep,djm,jmc,jv2}
% JV1 ; Henry ; \dots

\begin{document}

\maketitle
% \tableofcontents
\begin{abstract}
	
Many kinds of data are better characterized by graphs than vectors, including social networks, telecommunication grids, and brains.  It is often desirable to compare two or more of these graphs with one another.  Unfortunately, sometimes these graphs are unlabeled, meaning there is no known correspondence between vertices on one graph and vertices on another graph. In such scenarios, making comparisons is relatively difficult, although two general strategies are possible.  First, one can compute a number of ``graph invariants,'' that is, scalar measures of graphs that are invariant to vertex permutations.  Second, one can solve the graph isomorphism problem (GIP), effectively aligned graphs with one another, as close as can be aligned.  Because no known algorithm can solve GIP in polynomial time, one is often forced to use approximate solutions.  We provide several approximations to GIP, and show how performance scales with computational time.  We compare various approaches, using somewhat idealized simulations, as well as simulations based on real brain-graph data, and finally real human brain-graph data.

\end{abstract}

\section{Introduction}

In the current digital age, swaths of data are collected in myriad diverse fields ranging from astronomy to zoology.  While these data are well characterized by networks, or graphs, the vast majority of statistical inference techniques, including the recent advances in machine learning, tend to assume the data live in finite dimensional Euclidean space.  Thus, inference techniques designed to specifically to address collections of graphs could potentially yield fruitful results across many domains.  Furthermore, these collections of graphs often lack vertex labels, meaning no known mapping from vertices in any one graph to any other graph, is available.  In such scenarios, the graphs are called ``unlabeled,'' and this is the scenario of interest for this manuscript.  Consider, for example, two food networks in different jungles.  Both might have lions and tigers and bears (oh my), so it is easy to compare across the jungles.  On the contrary, if one jungle only has the birds and the bees, one might still be interested in comparing the food networks, but a bird might play the same role in the second jungle as the lion does in the first, making comparisons across jungles more complicated.



The exploitation task of investigation for this manuscript is that of unlabeled graph classification.  More specifically, we assume that we have observed some number of unlabeled graph/class pairs, and we want to classify a novel unlabeled graph into its appropriate class. Unfortunately, because we do not directly observe the complete graphs, rather, only the unlabeled version of each graph, our classifiers must somehow deal with this additional complication.

We consider two general approaches.  First, one can compute a large number of statistics on unlabeled graphs that are invariant to isomorphisms.  That is, if $g$ is a graph, and $\mt{g}$ is another graph isomorphic to $g$, then any function that satisfies $f(g)=f(\mt{g})$ is called a ``graph invariant.''  For example, the number of edges or triangles in a graph is invariant to isomorphisms.  By computing a large number of these graph invariants, one effectively projects the graph into finite Euclidean space, at which time one can apply standard machine learning tools.  A second approach under consideration involves first (approximately) solving GIP for each test graph, and then applying \emph{labeled} graph classification algorithms, such as those developed in \cite{??}.

Neither of these approaches are likely to dominate the other in general, as both have rather severe limitations.  The graph invariant approach lacks much rigorous theoretical support.  For instance, it remains an open question for which distributions can the graph invariant approach achieve the Bayes optimal classification rate (except in trivial cases for which the generative model is a function of the particular graph invariants being used). However, many graph invariants can be computed quite quickly.  Further, graph invariants have both local and global properties, that is, they capture some aspect of the whole graph, by recursively considering local functions.  Finally, for certain exploitation tasks, one might desire to understand the relationship between certain graph invariants and the class (for instance, are transportation grids more efficient when more triangles exist?).  

On the other hand, if one first (approximately) solves GIP, then limiting results for many classifiers are readily available (upon stacking each adjacency matrix to project it onto finite Euclidean space).  Unfortunately, solving GIP is known to be NP incomplete \cite{??} (not known to be NP complete nor P).   This means that solving GIP remains quite computationally expensive, especially as $n$, the number of vertices in the graphs, gets large.

Several questions therefore arise for us, as a first investigation into unlabeled graph classification.  Are there distributions for which approximate GIP solutions yield good classification results? If so, can we establish any general rules under which approximating GIP is expected to perform relatively well?  Alternately, are there distributions for which the graph invariant approach outperforms the GIP approach?  How do the computations and performances for these two approaches scale with respect to one another?  In this study, we first describe precisely how one might proceed upon using either of the above two approaches.  Then, we conduct a number of in simulu experiments that we believe are informative with regard to some of these questions.  We conclude with some discussion and outlines for possible future work and applications.

\section{Methods} % (fold)
\label{sec:methods}

\subsection{Preliminaries} % (fold)
\label{sub:nomenclature}

% subsection nomenclature (end)

Let $G$ be a graph-valued random variable, $G: \Omega \mapsto \mc{G}$, where $\mc{G}$ is the set of all possible graphs, $g \in \mc{G}$ (and $\Omega$ is the universal sample space). A graph is assumed to be a triple, $(V,A,Q)$, where $V=\{V_i\}=V_i \, \forall i \in [n]=\{1,\ldots,n\}$ is the set of vertices, $A=\{A_{ij}\}$ is the adjacency matrix, each edge is binary (an easily relaxed assumption), and $Q$ is a permutation matrix (a matrix with a single one in each column and row, and zeros otherwise). Let $\mc{A}=\{0,1\}^{n \times n}$ be the space of all possible adjacency matrices, and $\mc{Q}$ denote the set of permutation matrices.

Further, let $Y$ be a binary-valued random variable, $Y: \Omega \mapsto \mc{Y} = \{0,1\}$, which can be easily generalized to any multinomial space (or continuous spaces with a bit more work).  Graphs and classes are sampled jointly and exchangeably from some unknown but true distribution, $\PP[G,Y; \theta] \in \mc{P}$, where $\mc{P}$ is the family of distributions under consideration, $\mc{P} = \{\PP[G,Y; \theta] : \theta \in \Theta\}$, $\theta$ is the parameter of the distribution, and $\Theta$ is the set of all possible parameters (in the sequel, we often drop the $\theta$ from $\PP[G,Y; \theta]$).  

Formally, we assume that  $(G,Y), \{(G_s,Y_s)\} \sim \PP[G,Y; \theta]$, where $(G,Y)$ is the graph/class test sample pair, and $\mc{T}_S=\{G_s,Y_s\}=\{(G_1,Y_1),\ldots, (G_S,Y_S)\}$ is the training sample set, where $S$ is the total number of training samples.  The goal is to impute the latent test sample class, $Y=y$, given the training data.  While we do not believe that $\PP[G,Y; \theta]$ is in fact true, we do believe that the true distribution (if one exists) can be approximated reasonably well by some distributions in $\mc{P}$, such that we have hope to achieve misclassification rates better than chance on at least some data sets.  

Unfortunately, we do not observe the graphs in their entirety, but rather, the permutation matrices are latent (we will often implicitly assume that vertices are observed or not, it is confusing).  That is, instead of observing $G_s=(A_s,Q_s)$, we observe only $A_s$.  Throughout, we will use the notation that $\mt{A}_s = Q_s A_s Q_s\T$, so, had we observed $\mt{A}_s$'s, then we would have observed all there is to perform the classification.  Note that graph invariants can therefore be defined as functions on $A$, not requiring $Q$ for computing their values.


\subsection{Graph invariant based classifiers} % (fold)
\label{sub:graph_invariant_based_classifiers}

Many graph invariants are available from the literature (see 
\url{http://en.wikipedia.org/wiki/Graph_invariant}
for a list of some of the most popular ones).   A subfield of random graph models, collectively called exponential family random graph models (ERGMs), typically models a graph by its triangles, $k$-stars, and the like \cite{??}.  Recent work from Pao et al. analyzed the power of various graph invariants for a special case of hypothesis testing \cite{??}.  Because of the close relationship between hypothesis testing and classification (see B\&D, pg. XXX \cite{??}), we consider the same set of graph invariants here.  In particular, we consider:
\begin{enumerate}
	\item size:  $\#(A)$ = the number of edges in the graph
	\item  maximum degree: $\del(G)= \max_{i \in [n]} d(V_i)$, where $d(V_i)$ indicates degree of vertex $i$, that is, the number of edges incident to $V_i$.
	\item maximum average degree: 	$MAD(G)=\max_{\Omega_G} \bar{d}(\Omega_G)$, where $\bar{d}(G)$ is the average degree of a graph, and $\Omega_G$ is all subgraphs of $G$.  Because computing $MAD(G)$ exactly is computationally taxing (how much???), we instead compute the maximum eigenvalue $MAD$ \cite{PaoPriebe10}.
	\item scan statistics: the $k$-th scan statistic is the maximum number of edges over all $k$-th order neighborhoods, $S_k(G) = \max_{i \in [n]} \#(\Omega_{N_k[i;G]})$, where $N_k[i;G]=\{j \in [n] : l(V_i,V_j) \leq k\}$, and $l(V_i,V_j)$ is the distance between any two vertices, that is, the minimum number of edges traversals between them.  We assume that $l(V_i,V_i)=0 \, \forall i$, and $l(V_i,V_j)=\infty$ if no path exists between $V_i$ and $V_j$.
	\item number of triangles:  a triangle exists whenever there is an edge from $V_i$ to $V_j$, one from $V_j$ to $V_k$, and one from $V_k$ back to $V_i$
	\item clustering coefficient: a measure that captures the degree to which nodes tend to cluster together
	\item average path length: typically defined by $\sum_{i,j} l(V_i,V_j)/(n^2-n)$, but to exclude the infinities, we let $\infty \mapsto 2 \max l(V_i,V_j)$.
\end{enumerate}

Let $\mb{f}(\cdot): \mc{G} \mapsto \Real^k$ be the function that takes a graph as input, and outputs a set of $k$ graph invariants.  Having defined such a function, one can then use standard machine learning algorithms to perform classification.  We plug the results into a ``standard'' suite of machine learning tools: (i) a linear classifier---linear discriminant analysis (LDA), (ii) a quadratic classifier---quadratic discriminant analysis (QDA), (iii) a support vector machine (SVM), (iv) random forests (RF), and (v) $k_n$ nearest neighbor classifier.  Note that the dimensionality of the data is quite large ($\mc{O}(n^2)$), we first reduce the dimensionality of the data using principal components analysis (PCA).  The dimensionality of the data to keep is chosen using the method of \cite{??}.

\subsection{Likelihood based approach} % (fold)
\label{sec:lik}

The Bayes optimal classifier is:
\begin{align}
	\mh{y} = \argmax_{y \in \{0,1\}} \PP[G,Y] = \argmax_{y \in \{0,1\}} \PP[G | Y] \PP[Y]
\end{align}
where $\PP[G|Y]$ is the \emph{likelihood} of observing a graph given its class, and $\PP[Y]$ is the prior probability of each class.  By obtaining estimates of these two terms, one can build a plugin classifier:
\begin{align}
	\mh{y} = \argmax_{y \in \{0,1\}} \mhP[G | Y] \mhP[Y]	
\end{align}
where $\mhP[G|Y]$ and $\mhP[Y]$ are the plugin estimates of the likelihood and prior, respectively.  Estimating the prior is trivial, one can simply use the maximum likelihood estimator, $\mh{\pi}=\mhP[Y=1]$ and $1-\mh{\pi}=\mhP[Y=0]$.  The likelihood term, however, is more complex.  It can be expanded:
\begin{align}
	\PP[G|Y] = \PP[A,Q | Y] = \PP[A | Y] \PP[Q]
\end{align}
as the adjacency matrix, $A$, and permutation matrix $Q$, are conditionally independent given the class label, $Y$.  Further, the permutation matrix is assumed to be independent of the class label.  Thus, if $Q$ were observed, then estimating $\PP[A|Y]$ would be all there is to it; unfortunately, $Q$ is latent, and thus must be imputed for each example.  

To proceed, we define an explicit likelihood model, which we will use in the sequel.  Because each element of the adjacency matrix is Binary, edges are Bernoulli random variables.  We assume that each edge is independent, but not identical.  If the permutation matrix was known to be the identity matrix, we would have:
\begin{align}
\PP[A,Q|Y] = \prod_{ij} \text{Bernoulli}(a_{ij}; \eta_{ij}^y)
\end{align}
However, because the permutation matrix is in general, not the identity, we instead have:
% , yielding the following model:
% The likelihood of a new graph, $G=(A,Q)$ is therefore a prod defined as follows: %. For the adjacency matrix, we assume here the independent edge model, that is:
\begin{align}
\PP[A,Q|Y] = \prod_{ij} \text{Bernoulli}\bigg(\sum_{ij} q_{ij} q_{ji} a_{ij}; \eta_{ij}^y\bigg) \PP[Q]
% \eta_{ij}^{\sum_{ij} q_{ij} q_{ji} a_{ij}}
 % \prod_{ij} \PP[A_{ij}, Q | Y]	= \prod_{ij} \text{Bernoulli}(a_{ij}; \eta_{ij}^y) = \prod_{ij} \big(\eta_{ij}^y\big)^{a_{ij}'}\big(1-\eta_{ij}^y\big)^{1-a_{ij}'}.
\end{align}
The permutation matrix has the following distribution:
\begin{align}
	\PP[Q] = \sum_{Q \in \mc{Q}} \delta_Q w_{Q}
\end{align}
where $\delta_Q$ is an indicator function taking unity value whenever $Q'=Q$ and zero otherwise, and $w_Q$ is the likelihood of any particular permutation matrix, where $w_Q\leq 0$ and $\sum_{Q}w_Q'=1$.  The simplest assumption is that each permutation matrix is equally likely, that is: $w_Q'=1/n!$, because $n!$ is the number of $n\times n$ element permutation matrices.  

The log-likelihood of $\PP[A,Q|Y]$, ignoring the $\PP[Q]$ term is:
\begin{align}
	\mc{L}(\mb{\eta},Q)  = \sum_{ij} \bigg\{\Big[\sum_{ij} q_{ij} q_{ji} a_{ij} \ln \eta_{ij}\Big] + \Big[ \Big(1+\sum_{ij} q_{ij} q_{ji} a_{ij}\Big) \ln (1-\eta_{ij})\Big]\bigg\}
\end{align}
The goal is then to maximize the log-likelihood with respect to both $\mb{\eta}$ and $Q$:
\begin{align}
	(\mh{\eta},\mh{Q}) = \argmax_{\mb{\eta} \in (0,1)^{n\times n} ,Q \in \mc{Q}} \mc{L}(\mb{\eta},Q),
\end{align}
which is difficult in part due to the constraint on $Q$.  But one can relax that constraint to allow $Q$ to be any doubly-stochastic matrix, which is a natural relaxation, given that permutation matrices are the extreme points of the set of doubly stochastic matrices.  A matrix is said to be doubly stochastic if both all the rows and all the columns sum to unity.  Thus, to maximize the log-likelihood, we have:
\begin{align}
	(\mh{\eta},\mh{Q}) \approx \argmax_{\eta \in  (0,1)^{n\times n}, Q \in \mc{D}} \mc{L}(\mb{\eta},Q),
\end{align}
where $\mc{D}$ is the set of doubly stochastic matrices.  Using Lagrange multipliers, we can rewrite \label{eq:mle}:
\begin{align}
	(\mh{\eta},\mh{Q}) \approx \argmax_{\eta \in  (0,1)^{n\times n}} \mc{L}(\mb{\eta},Q) - \lambda (1 - \tfrac{1}{n}\mb{1}\T Q) - \lambda (1 - \tfrac{1}{n}\mb{1}\T Q\T),
\end{align}
where the additional terms impose the constraint that the columns and rows sum to one, respectively.  Now, the likelihood term is concave in $\eta$, and the constraints are linear, and therefore concave in $Q$, thus, one can use a coordinate ascend strategy, optimizing $Q$ followed by optimizing $\eta$, until convergence. This ignores the $Q$ terms in the likelihood.  Perhaps somebody (else) could figure out how to (at least approximately) account for those.  Regardless, this is an approximate solution to finding the most likely permutation matrix for each sample, as well as the independent edge probabilities.   


\subsection{Assignment problem approach} % (fold)
\label{sub:graph_isomorphism_approach}


Instead of the likelihood based approach outlined above, one could consider the permutation matrices ``nuisance parameters,'' and construct \emph{some} way to estimate them, and plug that value in, assuming it is ``correct.'' Note that the disadvantage of this approach relative to the likelihood based approach is that it is less ``natural'' from a statistical point of view, for example, modeling averaging is not quite as obvious, and incorporating prior information might also be less obvious.  The advantage, however, is that much work as already been devoted to solving assignment problems, and we can therefore piggyback on the shoulders of giants.  %In this case, we piggyback on the shoulders of Marguerite Frank and Phil Wolfe, who developed the Frank-Wolfe (FW) algorithm for solving quadratic programming problems with linear constraints.  

More specifically, assume we have the same model assumed above in Section \ref{sec:lik}, and assume that $\eta$ is known, such that we must only estimate the permutation matrix.  This problem is closely related to the graph isomorphism problem (GIP), in which, given any pair of graphs, one searches for the permutation matrix that makes them as similar as possible:
\begin{align} \label{eq:a}
	\mh{Q} = \argmin_{Q \in \mc{Q}} \norm{Q A Q\T - B}^2_F
\end{align}
where $A$ and $B$ are two arbitrary adjacency matrices, $\norm{\cdot}_{F}$ indicates the Froebenius norm, $\norm{x}_F=\sum_{ij} \sqrt{x_{ij}^2}$.  Unfortunately, because the set of permutation matrices, $\mc{Q}$, is discrete, optimizing over them is difficult.  In fact, solving GIP is known to be $NP$ incomplete (meaning it is not known to be in either $P$ or $NP$) \cite{??}.  Therefore, instead of trying to solve this exactly, we make approximations to make it easier.  

\subsubsection{Linear assignment problem solution} % (fold)
\label{ssub:linear_assignment_problem_solution}

Solving \eqref{eq:a} is difficult for two reasons: (i) the objective function is not necessarily convex, and (ii) the constraints are nonlinear.  That the objective function is not necessarily convex is clear by virtue of computing the Hessian of the argument in \eqref{eq:a} with respect to $Q$:
\begin{align}
	\nabla_Q^2 \mc{L} =  B \otimes A + B\T \otimes A\T,
\end{align}
where $\otimes$ indicates the Kronecker product, which is not positive definite (in general).  To resolve non-convexity, we can simply drop one of the $Q$'s from \eqref{eq:a}.  The implication of this is that instead of permuting both rows and columns, we only permute one or the other.  While this may seem like a totally ridiculous thing to do, crazier things have worked.  Given this simplification, the argument is not merely convex, but it is quadratic in $Q$, making solving it exactly feasible (independent of the constraints).  

To deal with the nonlinear of the constraints, we relax them to require that $Q$ merely be a doubly stochastic matrix.  A matrix is doubly stochastic if both its rows and columns sum to unity.  This is a ``natural'' relaxation in the sense that permutation matrices are doubly stochastic matrices, in fact, permutation matrices are the extreme points of the set of all doubly stochastic matrices, which we denote by $\mc{D}$.  

Given the above two relaxations, we instead try to solve:
\begin{align} \label{eq:a}
	\mh{Q} = \argmin_{Q \in \mc{D}} \norm{A Q\T - B}_F^2 = \argmin_{Q} \norm{A Q\T - B}_F^2 - \lambda (1 - \tfrac{1}{n}\mb{1}\T Q) - \lambda (1 - \tfrac{1}{n}\mb{1}\T Q\T),
\end{align}
where the second equality follows from replacing the linear constraint with its equivalent Lagrange multipliers.  If $\mh{Q}$ is not a permutation matrix, then projecting it onto its nearest permutation matrix is linear assignment problem:
\begin{align}
	\mh{Q}_{LAP} = \argmin_{Y \in \mc{Q}} \norm{\mh{Q} - Y}_F^2 = \argmin_{Y \in \mc{Q}} Y\T \mh{Q}
\end{align}
which can be solved using the Hungarian algorithm in $\mc{O}(n^3)$, as shown in the below lemma. XXX: do we need this additional step? seems like we should not need it, but i don't see how.

\begin{Lem}
  Given $Y \in \Real^{n \times n}$, the projection of $Y$ onto $\mc{Q}_n$ can
  be obtained by solving the linear assignment problem:
  $$ 
  \mh{W}= \argmin_{W \in \Pi_n} \norm{ W-Y}_F = \argmax_{W \in \mc{D}_n}  tr(YW\T)
  %   \Pi_n }\rbrace \equiv {\text{\rm arg }} \max_W \lbrace{ tr(YW^t)
  %   \mid W \in \mathcal D_n }\rbrace .
  $$
  Proof: Since $W \in \Pi_n \subset \mathcal O$, we have
  \begin{align}
  \norm{W-Y}_F^2 &= tr( (W-Y)\T (W-Y)) \\
  &= (\norm{Y}_F^2 + \norm{I}_F^2) - 2tr( Y W\T ). \qed
  \end{align}
\end{Lem}

(XXX: I don't know the trace rule.  perhaps somebody can prove that to me, or point it out?)


% subsubsection linear_assignment_problem_solution (end)


\subsubsection{Bilinear assignment problem solution} % (fold)
\label{ssub:bilinear_assignment_problem_solution}


A different strategy is to simply neglect the non-convexity of the argument, and merely relax the constraints to be linear.  This results in finding a local maximum, but if the initial conditions are chosen wisely, then this local maximum might be better than the global maximum found in the linear assignment approach.  Furthermore, it is desirable to permute both the rows and columns together, and the linear assignment approach only permutes one or the other.  Thus, making only the constraint relaxation, we have:
\begin{align} \label{eq:D}
	\mh{Q} = \argmin_{Q \in \mc{D}} \mc{L}(Q) = \argmin_{Q \in \mc{D}} \norm{Q A Q\T - B}_F^2 
\end{align}
The Frank-Wolfe (FW) algorithm is a successive linear programming algorithm specifically designed to solve quadratic programming problems with linear constraints \cite{??}.  Here, the problem is not quadratic in $Q$, rather, it is a 4th order polynomial function of $Q$.  Thus, FW will not (in general) achieve the global optimal.  Regardless, it proceeds as follows:
\begin{Alg}[Frank-Wolfe Algorithm (FW)]
Given $ Q^{(1)} \in \mathcal D_m $.
%\roster \widestnumber\item{xxxxxxxx} \raggedright
\item {Step 0:} Let $k$ = 1.

\item {Step 1:} Let $\nabla_Q^{(k-1)} = A Q^{(k-1)} B\T + A\T Q^{(k-1)} B$.  

\item {Step 2:} Compute $W^{(k)}$  by solving the
linear assignment problem:
% $$
\begin{align}
	W^{(k)} = \argmin_{W^{(k)} \in \mc{D}} %{\text{\rm arg }} \min_{W^{(k)}}
	% \lbrace { 
	\sum_{i,j=1}^m 
	\left( 
	\nabla_Q^{(k-1)}\circ W^{(k)}
	 \right)_{ij}
	% }\rbrace	
\end{align}
% $$
\item {Step 3:} Let $d^{(k)} = W^{(k)} - Q^{(k-1)}. $
\item {Step 4:} Find $\alpha^{(k)} \in [0,1]$ such that
$$\alpha^{(k)}=\argmin_{\alpha^{(k)}}
\mc{L} (Q^{(k)} + \alpha^{(k)} d^{(k)}).
$$
\item {Step 5:} Let

$$ Q^{(k+1)} = Q^{(k)} + \alpha^{(k)} d^{(k)}. $$
\item {Step 6:} 
If $\norm d^{(k)}_{P} \norm \le \epsilon$ or $\alpha^{(k)}=0$ or $k=k_{\max}$
then stop; else $k$ = $k$ + 1 and go to step 1.
\end{Alg}



\begin{Rem}
%\item This implementation is based upon a stop rule that uses the
%norm of the gradient projected onto the doubly stochastic constraint set;
%consequently, any solution produced by the above algorithm satisfies
%the first-order necessary optimality conditions.
%Other stop rules can be used that require less work than the projection and
%will be considered when the general QAP is solved later in the paper.

\begin{enumerate}
\item The above algorithm is effectively Netwon's method, applied to the this problem, with linear constraints.  
\item With each step, $Q^{(k)}$ is not (in general) a permutation matrix.  
\item One can project $Q^{(k)}$ into the space of permutation matrices at each step, or one can merely project the final $Q^{(k_{max})}$ (see below Lemma).
\item Step 3 is a ``line search;'' however, as the objective function is linear, the optimum $\alpha$ can be found exactly.
%\item It is well-known that the algorithm is quadratically
%convergent when in the neighborhood of a KKT point.
\end{enumerate}
\end{Rem}


% subsubsection bilinear_assignment_problem_solution (end)

% subsection graph_isomorphism_approach (end)


\subsection{Simulations} % (fold)
\label{sub:simulations}

In all the below simulations, as mentioned above, we assume the data is sampled exchangeably, that is: $(G,Y), \{(G_s,Y_s)\} \overset{exch.}{\sim} \PP[G,Y; \eta]$, where $G=(V,A,Q)$.  Further, we assume an independent edge model
\begin{align}
\PP[A,Q | Y] &= \PP[A|Q,Y] \PP[Q] = \prod_{i,j \in [n]} \text{Bernoulli}\left(\sum_{ij} q_{ij} q_{ji} a_{ij}; \eta_{ij}^y\right) \sum_{Q\in \mc{Q}} \delta_Q/n! \\
&=  \prod_{i,j \in [n]} \text{Bernoulli}\left(\mt{a}_{ij}; \eta_{ij}^y\right) \sum_{Q\in \mc{Q}} \delta_Q/n!
\end{align}
where $\mt{a}_{ij} = \sum_{ij} q_{ij} q_{ji} a_{ij}$.  This means that the class-conditional distributions are just the matrices $\eta^0$ and $\eta^1$, where $\eta^y=\{\eta_{ij}^y\}$. Moreover, the class-conditional difference is therefore also a matrix, $\delta=\{\delta_{ij}\}$, where
\begin{align}
	\delta_{ij} = | \eta_{ij}^1-\eta_{ij}^0|.
\end{align}
We also define the relative class-conditional differences
\begin{align}
	\delta_{ij}^r = \left| \frac{\eta_{ij}^1}{\eta_{ij}^1 (1-\eta_{ij}^1)} - \frac{\eta_{ij}^0}{\eta_{ij}^0 (1-\eta_{ij}^0)} \right|
\end{align}
which normalizes differences by their magnitude, similar to z-scores for comparing Gaussian distributed random variables. For each of the below simulated experiments, we generated $S=100$ training samples, half from each class, and two test samples, one from each class.  



\paragraph{Homogeneous kidney-egg simulation} % (fold)
\label{par:homogeneous_kidney_egg_simulation}

For the homogeneous kidney-egg simulations, we assume $n=10$, so $\eta^y \in (0,1)^{n\times n}$.  We further assume a stochastic block model, with two blocks, for both classes.  For class 0, $\eta_{ij}^0=q_0=0.25$ for $i,j \in [3]$, and for class 1, $\eta_{ij}^1=q_1=0.75$ for $i,j \in [3]$.  In both classes, $\eta_{ij}=p=0.5$ for $i,j \in \{4,\ldots 10\}$.  See Figure \ref{fig:models} (top row) for a graphical depiction of this model.  


% paragraph homogeneous_kidney_egg_simulation (end)

\paragraph{Heterogeneous simulation} % (fold)
\label{par:homogeneous_kidney_egg_simulation}

For the heterogeneous simulations, we assume $n=10$, so $\eta^y \in (0,1)^{n\times n}$.  We further assume that each $\eta_{ij}^y \overset{iid}{\sim}$Uniform$(0,1)$.  See Figure \ref{fig:models} (bottom) row for a graphical depiction of this model.



% paragraph homogeneous_kidney_egg_simulation (end)






% subsection simulations (end)

\section{Results} % (fold)
\label{sec:results}

% section results (end)

\subsection{General QAP Test Problems}
%\label{sec:genqap}
%\begin{enumerate}
%\item  QAPLIB - Burkard, Karisch, Rendl [1994] \cite{burkard:1994}
%\item  $n$: 5 - 128
%\end{enumerate}
To illustrate the effectiveness of FW 
we give the performance of the algorithm on 16 sample problems.  The below table gives the optimum value found by FW with 1, 2, 3, and 100 random starting points   and compared 
to the best known solution as given in column 2 and the Path algorithm  \cite{Path:2009}, column 7.   The starting point for FW, $X^{(0)}$ is chosen to be
$$X^{(0)}=\frac {1}{2n} + \frac{1}{2} S$$
where $S$ is a doubly stochastic matrix as computed by running Sinkhorn balancing on a uniform $[0,1]$ matrix.

\begin{table}[htdp]
\caption{Comparison of Frank-Wolfe with Minimum Solution and Path Algorithm}
\begin{center}
\begin{tabular}{|r|r||r|r|r|r|r|}
\hline
Problem  &   Min    & FW$_{100}$&FW$_{3}$&FW$_{2}$&FW$_{1}$&Path\\
\hline
    chr12c &   11156 &   12176 &   13072 &   13072 &   13072 &   18048\\
    chr15a &    9896 &    9896 &   17272 &   17272 &   27584 &   19086\\
    chr15c &    9504 &   10960 &   14274 &   14274 &   17324 &   16206\\
    chr20b &    2298 &    2786 &    3068 &    3068 &    3068 &    5560\\
    chr22b &    6194 &    7218 &    7876 &    7876 &    8482 &    8500\\
    esc16b &     292 &     292 &     294 &     294 &     320 &     300\\
     rou12 &  235528 &  235528 &  238134 &  253684 &  253684 &  256320\\
     rou15 &  354210 &  356654 &  371458 &  371458 &  371458 &  391270\\
     rou20 &  725522 &  730614 &  743884 &  743884 &  743884 &  778284\\
    tai10a &  135028 &  135828 &  148970 &  157954 &  157954 &  152534\\
    tai15a &  388214 &  391522 &  397376 &  397376 &  397376 &  419224\\
    tai17a &  491812 &  496598 &  511574 &  511574 &  529134 &  530978\\
    tai20a &  703482 &  711840 &  721540 &  721540 &  734276 &  753712\\
    tai30a & 1818146 & 1844636 & 1890738 & 1894640 & 1894640 & 1903872\\
    tai35a & 2422002 & 2454292 & 2460940 & 2460940 & 2460940 & 2555110\\
    tai40a & 3139370 & 3187738 & 3194826 & 3194826 & 3227612 & 3281830\\
    \hline
\end{tabular}
\end{center}
\label{tab:fwpath}
\end{table}%


\subsection{Simulation results} % (fold)
\label{sub:simulation_results}

The performance degradation due to the application of {\bf lp.assign} in {\bf R} package {\bf lpSolve} (with $Q=I$ specifying starting point) is
from $\hatL = 0.0476 \approx L^* = 1-F_{Binomial(9,0.25)}(4) = 0.04892731\dots$ to $\hatL = 0.28855$. So better-than-chance classification is
achieved for our unlabeled scenario using this assignment algorithm, but performance is significantly degraded.


\subsubsection{homogeneous kidney egg simulations} % (fold)
\label{ssub:homogeneous_kidney_egg_simulations}


\begin{figure}[htbp]
	\centering		\includegraphics[width=1.0\linewidth]{../../../figs/graph_sims/unlabeled/homo_kidney_egg_params}
	\caption{model}
	\label{fig:models}
\end{figure}

\begin{figure}[htbp]
	\centering			\includegraphics[width=1.0\linewidth]{../../../figs/graph_sims/unlabeled/homo_kidney_egg_Lhats}
	\caption{Lhats for homo kidney egg}
	\label{fig:figs_Sims_unlabeled_LPvsFW500}
\end{figure}


\begin{figure}[!ht]
\centering \includegraphics[width=.9\linewidth]{../../../figs/graph_sims/unlabeled/homo_kidney_egg_obj}
\caption{objective function for simulation 1}
\label{fig:sim1_params}
\end{figure}


% subsubsection homogeneous_kidney_egg_simulations (end)



\subsubsection{heterogeneous simulations} % (fold)
\label{ssub:heterogeneous_simulations}

\begin{figure}[htbp]
	\centering		\includegraphics[width=1.0\linewidth]{../../../figs/graph_sims/unlabeled/hetero_params}
	\caption{hetero model}
	\label{fig:models}
\end{figure}

\begin{figure}[htbp]
	\centering			\includegraphics[width=1.0\linewidth]{../../../figs/graph_sims/unlabeled/hetero_Lhats}
	\caption{Lhats for hetero}
	\label{fig:figs_Sims_unlabeled_LPvsFW500}
\end{figure}


\begin{figure}[!ht]
\centering \includegraphics[width=.9\linewidth]{../../../figs/graph_sims/unlabeled/hetero_obj}
\caption{objective function for hetero simulation}
\label{fig:sim1_params}
\end{figure}

% subsubsection heterogeneous_simulations (end)

% subsection simulation_results (end)

% subsection latent_permutation_matrix_approach (end)





% subsection graph_invariant_based_classifiers (end)

% section methods (end)

\clearpage
\section{other} % (fold)
\label{sec:other}

% section other (end)




Finding the best alignment of vertices has been called the graph isomorphism problem (GIP) \cite{??}, and is known to be computationally equivalent to the quadratic alignment problem (QAP) \cite{??}.  Given to graphs, $A$ and $B$, the QAP can be formalized as follows:
\begin{align}
	\mh{P} = \argmin_{P \in \Pi} \norm{PAP\T - B}
\end{align}
where $\Pi$ is the set of all permutation matrices, that is, all matrices with a single one in each row and column, and zeros otherwise.  


We therefore investigate building classifiers on the space of collections of unlabeled graphs.  Specifically, we consider two distinct approaches. First, we build classifiers using ``graph invariants,'' that is, statistics of the graphs that are invariant to isomorphic translations.  Second, we first approximately solve the graph isomorphism problem,  





Data today are unlabeled graphs.  examples (mostly brain-graph examples)....

Previous work classifying unlabeled graphs..... Problem: consistency for what model?

Previous work classifying labeled graphs.... Problem: graph isomorphism

Previous work solving graph isomorphism problem.

Paper organization...

\section{Preliminaries} % (fold)
\label{sec:preliminaries}



Consider $(G,Y),\{(G_i,Y_i)\}_{i=1}^s \overset{iid}{\sim} \PP_{GY}$,
with classes $Y:\Omega \rightarrow \{0,1\}$ and
(labeled) graphs $G:\Omega \rightarrow \mcG_n$,
where $\mcG_n$ denotes the collection of simple (labeled) graphs on $V=[n]$.

NB:
We consider simple graphs -- unweighted, undirected, with no loops,
so the adjacency matrices are binary, symmetric, and hollow;
our connectome applications
may involve {directed, loopy, weighted, attributed, multi} graphs \ldots
(And I use directed loopy graphs in my example Section \ref{Example} \dots
perhaps we should use directed loopy throughout? or undirected unloopy? what say you, John???)

The collection
$\mcT \equiv \{(G_i,Y_i)\}_{i=1}^s$ is the training sample
and $s$ is the training sample size;
$G$ is the graph to be classified,
and $Y$ is the true but unobserved class label.
For simplicity, we will assume that the prior probability of class membership
$\pi \equiv P[Y=1]$ is known to be $1/2$,
and the class-conditional sample sizes $S_y \equiv \sum_{i=1}^s I\{Y=y\}$
are fixed ($s_y$) rather than random variables ($S_y$)
with $s$ even and $s_0=s_1=s/2$.

We consider the independent edge model (IE),
so that for $y \in \{0,1\}$ the class-conditional distribution $F_{G|Y=y}$
is parameterized by a (symmetric, hollow)
$n \times n$ matrix $P_y$ with entries $p_{y;u,v} \in [0,1]$.

(NB: We will eventually *generalize* the independent edge model to RDPG \ldots and beyond!
But first, herein, we will *simplify* to independent edge block model (IEBM), for mathematical expediency.)

For this IE model, the Bayes optimal classifier for observed graph $G$
(equivalently, for observed (symmetric, hollow) $n \times n$ adjacency matrix $A=A(G)=[a_{u,v}]$)
is given by
\begin{eqnarray}
g^*(G) &=& \arg\max_y \prod_{(u,v) \in {V \choose 2}} f(a_{u,v};p_{y;u,v}),
\end{eqnarray}
where the Bernoulli probability $f(a;p)$ is given by
%$f(a;p) = p I\{a=1\} + (1-p) I\{a=0\}$.
$f(a;p) = p^{a} (1-p)^{1-a}$.

Alas, the graph $G$ is not observed;
rather, we observe the {\em unlabeled} version.
(right. we don't observe the class label.
but here i mean ``unlabeled'' in the ``assignment problem'' sense.
too.)
That is, rather than observing the adjacency matrix $A$,
we observe $\tildeA \equiv QAQ^T$ for some unknown permutation matrix $Q$.
\\
(NB: sorry John.  because of all the $P$s, i'm gonna use $Q$ for permutation matrices.  deal with it?)

% section preliminaries (end)



\section{An Assignment Problem Application}

%``assignment problem'' applications:
%(1) block assignment;
%(2) vertex assignment;
%(3) combination.

(This is Assignment Problem Application \#1.
E.g., voxels = vertices,
and voxels have been morphed to anatomical regions as per JV's description.
If we assume for \#1 that this region assignment is perfect,
we have the within-region vertex assignment problem.
This is this \dots
we disussed adding distance $d(u,u')$ and assuming that true assignment is more likely for smaller distances;
we discussed relaxing the perfect region assignment to $P[u \in$ correct region$]$;
we discussed other generalizations?)


%Let $d_{\hatP}(\tildeA,A_i) = ||\hatP \tildeA \hatP^T - A_i||_F.$
%Let $g_{NN}(G;P') = \arg\min_y \min_{i:Y_i=y} d_{P'}(\tildeA,A_i).$

%The {\em nearest assignment neighbor classifier} for
%the observed $\tildeA \equiv PAP^T$
%is given by
%$$g_{NAN}(G) = \arg\min_y \min_{i:Y_i=y} ||\hatP_i \tildeA \hatP_i^T - A_i||_F.$$
%
%If $P$ is known,
%then
%the nearest neighbor classifier
%$$g_{NN}(G) = \arg\min_y \min_{i:Y_i=y} ||P \tildeA P^T - A_i||_F$$
%satisfies
%$L(g_{NN}) \rightarrow L_{NN}(\PP_{GY}) \leq 2L^*$ [Cover \& Hart].
%
%Let us consider
%$$g(G) = \arg\min_y \min_{i:Y_i=y} \min_{\hatP} ||\hatP \tildeA \hatP^T - A_i||_{F(\mcE)}.$$

Consider the observed $\tildeA \equiv QAQ^T$.
For $i \in [s]$, let
\begin{eqnarray}\label{assignmenteqn}
\hatQ_i = \arg\min_{Q'} ||Q'^T \tildeA Q' - A_i||_F.
\end{eqnarray}
For each pair $(u,v)$, let $\sigma_i(u,v)$ be the reassignment through $Q$ and $\hatQ_i$.
That is, entries $a_{u,v}$ in $A$ are out of place due to unlabeledness in $\tildeA$,
and the assignment minimization attempts to put them back into place;
$\sigma_i(u,v)$ is the result of this attempt -- the assignment provided by
$\hatQ_i Q A Q^T \hatQ_i^T$.




\defa\label{nac}
The {\em naive assignment classifier} for the observed $\tildeA$ is given by
\begin{eqnarray}
g(G;\mcT) = {\arg\max_y \max_{i:Y_i=y} \prod_{(u,v) \in {V \choose 2}} f(a_{\sigma_i(u,v)};p_{y;u,v})}.
\end{eqnarray}
\defb

Note 1:
The classifier $g(G;\mcT)$ presented in Definition \ref{nac}
assumes that the class-conditional edge probabilities $p_{y;u,v}$ are known;
however, these probabilities are not used in the assignment equation \ref{assignmenteqn}.
(They could be? But they're not!)

Note 2:
We could employ a plug-in classifier,
using estimates in both the classifier and the assignment;
e.g., $\hatp_{y;u,v} = (s_y)^{-1} \sum_{i:Y_i=y} a_{i;u,v}$
and $\overline{A}_y = (s_y)^{-1} \sum_{i:Y_i=y} A_{i}$.
However, we would want to use {\em smoothed} estimates in the classifier
(to avoid degeneracy when $\hatp_{y;u,v}$ equals 0 or 1)
and {\em unsmoothed} estimates in the assignment.
This complicates the evaluation analysis;
we leave this more complicated (and more realistic) investigation for the future.

Note 3:
The classifier $g(G;\mcT)$ presented in Definition \ref{nac}
uses only the single probability-maximizing training assignment for each class.
This could be generalized either in the assignment (using $\overline{A}_y$)
or by processing the collection
$\{\prod_{(u,v) \in {V \choose 2}} f(a_{\sigma_i(u,v)};p_{y;u,v})\}_{i:Y_i=y}$
with methods more elaborate than the simple maximum.

Note 4:
We could also use nearest neighbor classifier \dots but i think this would be less tractable.

The advantage of the classifier $g(G;\mcT)$ presented in Definition \ref{nac}
is that the assignment methodology and only the assignment methodology is on trial!
Better classifiers could be considered,
but I'm trying to design a tractable investigation of assignment methodologies \dots

%\subsection{A Simpler Model: IEBM$(n,p,\mcE)$}

Under IE,
the difference between
$F_{G|Y=1}$ and $F_{G|Y=0}$
is wholly captured by the collection of marginal ``signal edges'' probabilities
$$\mcE \equiv \{(u,v) \in {V \choose 2}: p_{0;u,v} \neq p_{1;u,v}\}.$$
This collection $\mcE$ might be all of ${V \choose 2}$.
We hereby simplify IE to independent edge block model (IEBM), for mathematical expediency.
Let $\mcE = {V' \choose 2}$ for a collection $V'$ of $1 \leq m \leq n$ vertices,
and define
IEBM$(n,p,m)$ to be the model $\PP_{GY}$ defined by class-conditional probabilities
$p_{0;u,v}=p \neq 1/2$ and $p_{1;u,v}=1-p$ for all $(u,v) \in \mcE$
and $p_{0;u,v}=p_{1;u,v}=1/2$ for all $(u,v) \in {V \choose 2} \setminus \mcE$.
(In this case, all signal edges are created equally and all noise edges are created equally.)
Notice that
IEBM$(n,p,m)$ requires that $\mcE$ is a block --
the signal edges consist of precisely the potential interconnections between a set of $m$ vertices.
%so that $\mcE$ can be thought of as an $m \times m$ submatrix 


Let $s=2$ (one single training observation from each class).
%Assume (for evaluation purposes, but not for algorithm development)
%that $p_{0;u,v}=p \neq 1/2$ and $p_{1;u,v}=1-p$ for all $(u,v) \in \mcE \subset {V \choose 2}$
%and that $p_{0;u,v}=p_{1;u,v}=1/2$ for all $(u,v) \in {V \choose 2} \setminus \mcE$.
In this case, since all signal edges are created equally and all noise edges are created equally,
the performance of the classifier $g(G;\mcT)$
is monotonic in the number of signal edges recovered by $\sigma_1$ and $\sigma_2$.

%If the estimates $\hatp_{y;u,v}$ are consistent (converge to $\hatp_{y;u,v}$ as $s \rightarrow \infty$),
%then both of these classifiers are consistent (converge to Bayes optimal);
%that is, $L(g) \rightarrow L(g^*) \equiv L^*$,
%where 

Let the random variable $L(g) \equiv P[g(G;\mcT) \neq Y| \mcT]$
be the probability of misclassification for classifier $g$
conditioned on the training sample [see DGL 1996].
Under IEBM$(n,p,m)$ with $s=2$, we have that $L(g)$ depends on only $n,p,m$.
Define $L^* \equiv L(g^*)$.

\thma
For $\PP_{GY} \in$ IEBM$(n,p,m)$,
$L(g|T=t) < L(g|T=t-1)$ for all $t \in [2m-1]$,
where
\begin{eqnarray}
T_i \equiv |\{(u,v) \in \mcE: \sigma_i(u,v) \in \mcE\}|
\end{eqnarray}
and
$$T \equiv T_1 + T_2.$$
\thmb

Proof:
(Proof of this (alleged) monotonicity requires but a simple modification to Henry's proof?)

So, for this simple case,
we need concern ourselves with only the performance of the assignment algorithm in terms of $T_i$.

\thma
$T_1 =^{\mcL} T_2$ for this simple case.
\thmb

Proof:
(By construction? it's suppose to \dots that's why i set up the class-conditional edge probabilities
 $p_{y;u,v}$ to be reflective about $1/2$ in $y$.)




\section{Performance Degradation}

What is the performance degradation due to unlabeled-ness?

Case I:

\thma
$P[\hatQ_i = Q]=1$ for all $i$ implies $L(g)=L^*$.
\thmb

Proof:
If $P[\hatQ_i = Q]=1$ for all $i$
(that is, if the assignment algorithm gets the {\em right} answer --
not to be confused with the {\em best} answer
in terms of the optimization)
then $T_1=T_2=|\mcE|$ and hence $L(g)=L^*$.


Case II:

How about when the assignment algorithm gets the {\em best} answer
in terms of the optimization?
Perhaps we can work this out -- a theorem/proof?

\thma
$L_{n,p,\mcE}(g) =$ some *identifiable* function of $n,p,\mcE$?
\thmb

According to my calculations,
the only tricky bit is $I\{T_i(g_1,g_2) = t\}$
given two graphs $g_1,g_2 \in \mcG_n$.
That is, given two graphs (no randomness),
$T_i$ either equals $t$ or it doesn't
(after (arbitrarily?) accounting for non-uniqueness
of assignment solution $\hatQ$).
This looks like i could do it for $n=3$.
But then the combinatorics get silly.
BUT: maybe this is one of those things for which
generating function folks
could derive a {\em generating function} \dots?
at least for my simple stochastic block model
class-conditional distributions?


After Case I
(the assignment algorithm gets the {\em right} answer --
not to be confused with the {\em best} answer in terms of the optimization)
and Case II
(the assignment algorithm gets the {\em best} answer in terms of the optimization),
we will investigate approximation assignment algorithms based on the trade-off between
(1) computational complexity and
(2) classification performance
(either $L(g)$ directly, or in terms of the distribution of $T_i$).


The Monte Carlo example presented below (Section \ref{Example})
demonstrates significant but not complete performance degradation
due to a particular algorithm
({\bf lp.assign} in {\bf R} package {\bf lpSolve}).




\section{Example}\label{Example}

A Monte Carlo experiment (20000 paired replications) demonstrates that,
(using {\bf directed loopy} graphs for simplicity)
with $n=10, p=0.25$, and $|\mcE|=9$
(where $\mcE$ is in fact a $3 \times 3$ block)
the performance degradation due to the application of
{\bf lp.assign} in {\bf R} package {\bf lpSolve}
(with $Q=I$ specifying starting point)
is from
$\hatL = 0.0476 \approx L^* = 1-F_{Binomial(9,0.25)}(4) = 0.04892731\dots$
to
 $\hatL = 0.28855$.
So better-than-chance classification is achieved for our unlabeled scenario
using this assignment algorithm,
but performance is significantly degraded.

NB:
should also report performance in terms of $T_i$.
and objective value at solution?

Code for this example is provided in the Appendix.

NB: LAP is not QAP; i'd be happy to have QAP R code \dots

\section{Proposal}

I propose that we investigate,
via theory, simulation, and experiment,
the trade-off between computational complexity and performance,
and also identify the relationship between
the explicit assignment objective function
and the exploitation task (classification) objective function.

Except for Cases
I
(the assignment algorithm gets the {\em right} answer --
not to be confused with the {\em best} answer in terms of the optimization)
and
II
(the assignment algorithm gets the {\em best} answer in terms of the optimization),
I'm not sure what we'll be able to prove.
Perhaps
\thma
LAP is as good as QAP $\iff$ model $\in$ IEBM?
\thmb
But we can do simulation analysis:
first, for my simple scenaro;
then, generalizing to (perhaps) approach experimental settings?
\\

NB: perhaps we should be doing hypothesis testing instead???


\section{20 statements}

\begin{verbatim}

	allow me to try to state some things that it seems we agree on, and some things that seem to be open questions for us, &c,
	with the hope of clarifying at least where we are, and where we'd like to be.  if anything i write below is incorrect, or somehow disagreed with, please state your qualms.

	(1) The "quadratic assignment problem" (QAP) is the following:  
	give two matrices, A and B,
	find a Q and P such that
	min ||QAP' - B||
	where both Q and P are permutation matrices.


	JMC: Actually, this is the bi-linear assignment problem for the QAP P=Q.
	The bilinear problem has the pleasant property that the relaxed version
	(replacing the permutation constraint with the doubly stochastic constraint)
	has its solutions on the vertices.  That is when you solve the
	bilinear assignment
	your are guaranteed to find a vertex.  The solution may be a local optimum and
	multiple starts may still be necessary.
	
	JoVo: modified first two points
	(1) The "bilinear assignment problem" (BAP) is the following:  
	give two matrices, A and B,
	find a Q and P such that
	min ||QAP' - B||
	where both Q and P are permutation matrices.

	(2) The Quadratic Assignment Problem (QAP) is the following:

	given two adjacency matrices, A and B,
	find a P such that
	min || PAP' - B||
	where P is a permutation matrix.
	thus, QAP is a constrained BAP.

	(3) the graph isomorphism problem is a QAP
	
	
	(2) QAP is NP-hard

	(3) The Graph Isomorphism Problem (GIP) is the following:
	given two adjacency matrices, A and B,
	find a P such that
	min || PAP' - B||
	where P is a permutation matrix.
	thus, GIP is a constrained QAP.

	(4) GIP has weird complexity, somewhere between NP-complete and P

	(4) The Linear Assignment Problem (LAP) is the following:
	given two adjacency matrices, A and B,
	find a P such that
	min || PA - B||
	where P is a permutation matrix.

	(5) LAP can be solved in O(n^3) by the hungarian algorithm

	(6) The Frank-Wolfe (FW) algorithm is an algorithm designed to *solve* quadratic problems with linear constraints, that is:
	min f(x) = 0.5* x' E x + h' x
	where x is in some feasible region defined by a set of linear constraints, that is Ax \leq b and Cx = d.

	(7) FW is an iterative algorithm that works as follows

	(i) initialize x with x_0
	(ii) compute the first order taylor expansion around f(x_k)
	(iii) compute the gradient of f(x_k), call that g(x_k)
	(iv) solve the following subproblem:
	min f(x_k) + g(x_k) xbar_k, where xbar_k is in the feasible region
	(v) compute the step size, lambda that solves
	min f(x_k + lambda (xbar_s - x_k)) subject to lambda is in (0,1)
	(vi) let x_{k+1} = x_k + lambda*(xbar_k - x_k), and let k=k+1

	we stop if ever g(x_k)=0 or lambda=0.

	(8) FW can be quite slow.  although the first few iterations are often fast.

	(9) doubly stochastic matrices have the following properties:

	(i) each row sums to 1
	(ii) each column sums to 1
	(iii) each element is a non-negative real number

	these can each be written as linear constraints (linear equalities, in fact).

	(10) the set of doubly stochastic matrices is a convex polytope, and is the convex hull of the set of permutation matrices

	(11) permutation matrices are square matrices with exactly one 1 in each row and one 1 in each column.  this can NOT be written as linear constraints.

	(12) thus, one can solve the following relaxation of GIP, called the Doubly Stochastic Approximation (DSA): 
	Yhat = min || YAY' - B||
	where Y is the set of doubly stochastic matrices.
	in particular, FW can solve this problem in linear time.

	(13) it is possible that the solution to DSA is in fact also a solution to GIP, because the extreme points of the set of doubly stochastic matrices are permutation matrices.

	(14) if Yhat is not a permutation matrix, then one can find the closest permutation matrix by  solving the following:
	min || Yhat - X ||
	which can be solved in polynomial time using the hungarian algorithm.  

	(15) we have found a case for which we can show in simulo:
	L* ~ E[L_{labeled}] << E[L_{LAP}] << 1/2

	(16) question 1: where does E[L_{QAP}] fit in for this simulation.  this is a question we can solve for very small graphs, perhaps analytically in some (overly) simplistic settings

	(17) question 2: where does E[L_{DSA}] fit in, where L_{DSA} is the misclassification rate when one solves DSA and then projects the answer onto the set of permutation matrices.  we can solve this problem analytically for some problems too.  for instance, for any problem that the solution to DSA is a permutation matrix, and we already know analytic results for the permutation case.

	(18) question 3: since DSA takes a long time, let DSA_k indicate the solution to DSA from the k-th iteration, projected onto the set of permutation matrices.  again, for a simple simulation, what does the plot of E[L_{DSA_k}] vs. k look like.

	(19) question 4: is it the case that the solution to DSA_1always equals the solution to LAP

	(20) question 5: how does conroy's algorithm fit into all of this? it seems that FW will eventually converge to the same solution no matter where it starts.  but, it seems FW is often stopped early (like after a single iteration).  so, perhaps conroy's algorithm is running FW many times, each starting in a different place, and each only iterating once?  i say multiple starts because that is the term conroy used in his email on 3/29, and multiple iterations doesn't make much sense to be, because FW is iterative, so conroy's code can't simply be FW, unless conroy was the first to propose to use FW to approximate GIP?

	i hope this missive at least clarifies what i believe and don't understand.  i also hope it inspires somebody else to correct any mistakes i currently have made, and conroy to fill me in (again) on what his algorithm is doing, as i still do not understand (much to my chagrin) (and my lack of understanding is likely due only to my failings).

	shabbat shalom to all,
	jovo
	
\end{verbatim}

\section{email from jmc}

\begin{verbatim}

	John Conroy <conroyjohnm@gmail.com>	 Sat, Sep 18, 2010 at 11:15 PM
	To: Carey Priebe <cep@jhu.edu>, joshuav <joshuav@jhu.edu>
	Hi Carey and Joshua,
	 Here's a rough draft description of the FW method for QAP.  I adapted from the
	original late 90's unpublished paper by Lou, Steve, and I.  I suspect that 
	once this is merged into the paper much can be cut out and some may be appropriate
	for an appendix.   I think it would be good to include the performance of the FW method
	on the QAP test set to justify the method. 

	Here's a couple of points relative to our discussion on Thursday:

	 1.  Indeed, the subproblem
	FW solves is a linearization of the quadratic function, a Taylor series about $X^{(k)}$.  
	This sub-problem's solution then produces a vertex which give a 
	search direction for the "exact line search."   
	2.  Furthermore, I am convinced that the
	relaxed QAP is NOT in general a convex optimization problem.  The Hessian is 
	kron(A,B)+kron(A',B') and in general this will not be a positive definite matrix.
	(A sufficient condition is for A and B to be symmetric positive definite or for them
	both to be symmetric negative definite.)

	I sent a copy of this draft to Lou and Steve as well
	 with some background where we intend to
	go next.

	Best regards,
	Johnny


\end{verbatim}


\section*{Appendix}

Code producing the example results presented in Section \ref{Example}.

\begin{verbatim}

library(lpSolve)
cuc = function(thisclass,myseed=1)
# Classification of Unlabeled Connectomes
{
set.seed(myseed)
n=10
nmc=10000
# directed, with loops
P1 = P2 = matrix(0.5,n,n) # class-conditional distribution specification
P1[1:3,1:3] = 0.25
P2[1:3,1:3] = 0.75
label = labeltilde = tie = tietilde = rep(0,nmc)
Qhat1lpobjval = Qhat2lpobjval = NULL
for(mc in 1:nmc)
{
G1 = matrix(rbinom(n^2,1,P1),n,n)
G2 = matrix(rbinom(n^2,1,P2),n,n)
if(thisclass == 1 )
 G  = matrix(rbinom(n^2,1,P1),n,n)
if(thisclass == 2 )
 G  = matrix(rbinom(n^2,1,P2),n,n)
Q = matrix(0,n,n)
diag(Q) = 1 # Q == I
Gtilde = Q %*% G %*% t(Q)
C1 = C2 = matrix(0,n,n) # cost
for(i in 1:n) for(j in 1:n)
 {
 C1[i,j] = sum(abs(Gtilde[i,]-G1[j,]))
 C2[i,j] = sum(abs(Gtilde[i,]-G2[j,]))
 }
Qhat1lp = lp.assign(C1)
Qhat2lp = lp.assign(C2)
Qhat1 = Qhat1lp$solution
Qhat2 = Qhat2lp$solution
Qhat1lpobjval[mc] = Qhat1lp$objval
Qhat2lpobjval[mc] = Qhat2lp$objval
sigma1 = t(Qhat1) %*% Gtilde
sigma2 = t(Qhat2) %*% Gtilde
# now ... classify G and Gtilde
p1 = prod( (P1^G) * ((1-P1)^(1-G)) )
p2 = prod( (P2^G) * ((1-P2)^(1-G)) )
p1tilde = prod( (P1^sigma1) * ((1-P1)^(1-sigma1)) )
p2tilde = prod( (P2^sigma2) * ((1-P2)^(1-sigma2)) )
if(p1>p2) label[mc]=1
if(p1==p2) tie[mc]=1
if(p1tilde>p2tilde) labeltilde[mc]=1
if(p1tilde==p2tilde) tietilde[mc]=1
}
return(list(label,tie,labeltilde,tietilde))
}

cuc1 = cuc(1)
cuc2 = cuc(2)

sum(cuc1[[1]])
sum(cuc1[[2]])
sum(cuc1[[3]])
sum(cuc1[[4]])
# [1] 9524
# [1] 0
# [1] 6986
# [1] 121

sum(cuc2[[1]])
sum(cuc2[[2]])
sum(cuc2[[3]])
sum(cuc2[[4]])
# [1] 476
# [1] 0
# [1] 2759
# [1] 117

(10000 - sum(cuc1[[3]]) - .5*sum(cuc1[[4]]) + sum(cuc2[[3]]) + .5*sum(cuc2[[4]]))/20000
# [1] 0.28855

# L*:
# > 1-pbinom(4,9,.25)
# [1] 0.04892731

\end{verbatim}

\newpage

\subsection{move this \dots}

Under IE,
the difference between
$F_{G|Y=1}$ and $F_{G|Y=0}$
is wholly captured by the collection of marginal ``signal edges'' probabilities
$$\mcE \equiv \{(u,v) \in {V \choose 2}: p_{0;u,v} \neq p_{1;u,v}\}.$$
\begin{eqnarray}
g^*(G) &=& \arg\max_y \prod_{(u,v) \in {V \choose 2}} f(a_{u,v};p_{y;u,v}) \\
       &=& \arg\max_y \prod_{(u,v) \in \mcE} f(a_{u,v};p_{y;u,v}),
\end{eqnarray}

If we estimate $p_{y;u,v}$ from the training data,
we may consider classifiers
\begin{eqnarray}
g_{NB}(G;\mcT) = \arg\max_y \prod_{(u,v) \in {V \choose 2}} f(a_{u,v};\hatp_{y;u,v})
\end{eqnarray}
and
\begin{eqnarray}
g_{\mcE}(G;\mcT) = \arg\max_y \prod_{(u,v) \in \mcE} f(a_{u,v};\hatp_{y;u,v}).
\end{eqnarray}
NB: requires *smoothed* estimates $\hatp_{y;u,v}$,
to avoid degeneracy when $\hatp_{y;u,v}$ equals 0 or 1.

The latter classifier, $g_{\mcE}(G;\mcT)$,
is the best we can hope for -- it considers the signal edges and only the signal edges;
the former can be swamped by noise from non-signal edges.

If the estimates $\hatp_{y;u,v}$ are consistent (converge to $\hatp_{y;u,v}$ as $s \rightarrow \infty$),
then both of these classifiers are consistent (converge to Bayes optimal);
that is, $L(g) \rightarrow L(g^*) \equiv L^*$,
where the random variable $L(g) \equiv P[g(G) \neq Y| \{(G_i,Y_i)\}_{i=1}^s]$
is the probability of misclassification for classifier $g$
conditioned on the training sample [see DGL 1996].
Note that $g_{\mcE}(G;\mcT)$ should dominate $g_{NB}(G;\mcT)$.


% \paragraph{Acknowledgments}

% \appendix
\clearpage

\bibliography{/Users/joshyv/Research/papers/meta/biblist}
\addcontentsline{toc}{section}{References}
%\bibliographystyle{apalike}
\bibliographystyle{ieeetr}
%\bibliographystyle{nature}


\end{document}
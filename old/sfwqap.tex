%Sunday, November 15, 1998 at 16:20
\input epsf
\documentclass{amsart}
%\documentstyle[12pt,twoside,fleqn,espcrc1]{article}
%{\catcode`\@=11\global\let\reset@font\relax}
% put your own definitions here:
%   \newcommand{\cZ}{\cal{Z}}
\usepackage{amsmath}
\usepackage{amssymb}
%\theoremstyle{break} \newtheorem{Cor}{Corollary}
\theoremstyle{plain} \newtheorem{Exa}{Example}[section]
%\theorembodyfont{\rmfamily}
\newtheorem{Rem}{Remark}[section]
\newtheorem{Alg}{Algorithm}[section]
\newtheorem{Thm}{Theorem}[section]
%\theoremstyle{marginbreak}
\newtheorem{Lem}{Lemma}[section]
%\newtheorem{Lem}[Cor]{Lemma}
%\theoremstyle{change}
%\theorembodyfont{\itshape}
\newtheorem{Def}{Definition}[section]
\title{Continuous~Methods  Quadratic
Assignment~Problem}
\author
{John~M.~Conroy,~Steven~G.~Kratzer \\ and Louis J. Podrazik}
%\footnote[\dag]{ Center for Computing Sciences,
%Institute for Defense Analyses, Bowie, MD 20715-4300. }}

%\\ \\ SRC-TR-96-xxxx (Draft)\\
\begin{document}
\bibliographystyle{plain}
\begin {abstract}
In this note we propose and analyze continuous methods
to solve quadratic assignment problems.
We formulate the
problem as a quadratic programming problem with non-negativity and
linear constraints.
We investigate the Frank-Wolfe algorithm
to solve this quadratic programming problem.  To demonstrate that this
simple approach is competitive 
we present the results of using this method to
solve standard test cases of the quadratic assignment problem,
an NP-hard problem.  As the problem is global optimization a multiple
start strategy is presented.  Using the QAP library test set the proposed method
finds solutions with a median performance of withing 5\% of the best solution
with 2 random starts.  
\end {abstract}
\maketitle
%\keywords SVD, Frank-Wolfe Method, SLP, matrix Procrustes problems
%\endkeywords
% \subjclass \endsubjclass
% ----------------------------------------------------------------------

%\footnote[1]{\raggedright
%The authors thank Dianne O'Leary of the
%Mathematics Department, Univ. of MD, College Park
%for discussions relating to the SVD approach presented here.}

\newcommand{\myepsfdef}[4]{%
    \begin{figure}[#1]
        \centerline{\epsfxsize=5.3000in \epsfbox{#2.eps}}
        \caption{#3}
        \label{#4}
    \end{figure}%
}
%
%  Big figure from eps file (whole page)
% #1 = file name without ".eps", #2 = caption and #3 = label
\newcommand{\bigepsfdef}[4]{%
    \begin{figure}[#1]
        \centerline{\epsfxsize=1.500in \epsfbox{#2.eps}}
        \caption{#3}
        \label{#4}
    \end{figure}%
}
%
%
\renewcommand{\topfraction}{1.0}
\renewcommand{\bottomfraction}{1.0}
\renewcommand{\textfraction}{0.0}
%\setlength{\textfloatsep}{2\baselineskip}

% \input mydefs
% \input epsf
%\UseAMSsymbols\TagsOnRight\TopOrBottomTagsOnSplits
% ----------------------------------------------------------------------
%\shortauthor{Conroy, Kratzer and Podrazik} \shorttitle {Continuous Methods for QAP}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{Introduction}
Linear assignment (LAP) and quadratic
assignment problems (QAP) arise frequently in facility location
problems in operations research (OR).  The algorithm given in
this paper are motivated by a matrix formulation of assignment
problems.  This formulation, we will see later are equivalent to
the common formulations found in OR literature \cite{Pard:Book}.
Informally, a linear assignment problem is to find a permutation
such that when the columns of $A$ are permuted the resulting
matrix is as near as possible to $B$, given two $m\times n$
matrices $A$ and $B.$   We now define some notation, which will allow us
to formally define the  LAP and QAP and expose some of the
structure of these problems.
\begin{Def} [Notation] \hskip 4in
\begin{enumerate}
\item $\norm A \norm_F = \left( \sum_{i,j} A_{i,j}^2 \right)^{\frac{1}{2}}$
\item Hadamard product of two matrices $A,B$:
$(A \circ B)_{ij} = a_{ij} b_{ij}$.
\item $\Pi_n \subsetneq E^{n \times n}$ space of permutation matrices,
the set of zero one matrices with exactly one 1 in each row and column.
\item $\Pi_{n,m} \subsetneq E^{n \times m}$ space of partial permutation matrices
\item $\mathcal D_n \subsetneq E^{n \times n}$ space of doubly stochastic matrices
\end{enumerate}
\end{Def}

We now give the definition of the assignment problem, which is a special
type of linear program.
\begin{Def} [Assignment Problem (AP)]
Given $C \in E^{n \times n}$

$$
\max_{X \in \mathcal D_n} \sum_{i,j=1}^n c_{ij} x_{ij}
=
\max_{X \in \mathcal D_n} \sum_{i,j=1}^n (C \circ X)_{ij}
$$
\end{Def}

We note that the LAP as defined is to find a doubly stochastic matrix
which maximizes the above sum.  Since the objective function is
linear we know that the optimum will occur on a vertex of constraints
set.  The set vertices of the doubly stochastic matrices, $\mathcal D_n$, is
the set of permutation matrices, $\Pi_n.$

Conversely, the constraint set for the quadratic assignment problem (QAP)
will be the $\Pi_{n,m}.$  Later in the paper we will considered
a relaxed version of the QAP which has $\mathcal D_{n,m}$ as the
constraint set.

\begin{Def} [Quadratic Assignment Problem (QAP)]
Given $A \in E^{n \times n}$, $B \in E^{m \times m}$
$$
\min_{X \subsetneq \Pi_{n,m}}
\sum_{i,j=1}^n \sum_{k,l=1}^m a_{ij} b_{kl} x_{ik} x_{jl}
=
\min_{X \subsetneq \Pi_{n,m}}
\sum_{i=1}^n \sum_{j=1}^m (AX \circ XB)_{ij}
$$
\end{Def}

In the next section we reformulate the LAP and the QAP as matrix
optimization problems.
\section{The Matrix Procrustes Problem \& Preliminaries}

We now need to define a bit more notation which we will need to help
reformulate the LAP and QAP.

\begin{Def} [Notation] \hskip 4in
\begin{enumerate}
\item Given a matrix $A \in E^{n \times m}$, the
stacking operator $s(.)$ creates an $mn$-vector $s(A)$ obtained by
stacking the columns of $A$ on top of each other.
\item The $m$-vector of all ones is $e_m$.
\item $I_{m}$ denotes an identity matrix of size $m.$ 
\item Given $x \in E^n$, the diagonal matrix whose diagonal elements are $x$
is denoted $diag(x)$.
\end{enumerate}
\end{Def}

We now consider the one-sided permutation constrained matrix Procrustes problem and
observe the well-known result that
it is equivalent to solving a linear assignment problem
in the case of permutation matrix constraints.

\begin{Def} [One-sided Matrix Procrustes Problem]
Given $A,B \in E^{m \times n}$,
find the permutation matrix $X\in E^{m \times m}$ to
$$\min_{X \in \Pi_m} \norm A-XB \norm_F . $$
\end{Def}

\begin{Lem}[]
The one-sided matrix permutation constrained Procrustes problem is equivalent to solving a linear assignment problem (LAP)
with cost assignment matrix $AB^t$, that is,
$$
\min_{X \in \Pi_m} \norm A-XB \norm_F
\equiv
\max_{X \in \Pi_m} tr( AB^t X^t )
= \max_{X \in \Pi_m}
\sum_{i,j=1}^m \left({ AB^t \circ X }\right)_{ij} .
\label{ LAPequiv}
$$

Proof:
Since $X^t X = I$, we have
$$ \aligned
\norm A-XB \norm^2_F &= tr \left({ (A-XB)^t (A-XB) }\right) \\
% &= tr \left({ A^t A + B^t B -2 BA^t X }\right) \\
&= \norm A \norm^2_F + \norm B \norm^2_F -2 tr( AB^t X^t ).
\endaligned
$$ Furthermore, $$ \aligned tr( AB^t X^t ) &= tr( I_n (AB^t)
I_n X^t ) \\ &= \langle e_m, (AB^t \circ X ) e_m \rangle =
\sum_{i,j=1}^m \left({ AB^t \circ X }\right)_{ij} . \qed
\endaligned
$$
\end{Lem}

Similarly, we have
\begin{Lem}[]
The quadratic assignment problem is equivalent to the following minimization problem
$$\min_{X \in \Pi_m} \norm A-XBX^{t} \norm_F . $$

\end{Lem}

In order to formulate the QAP as a constrained optimization we 
and to efficiently compute the gradient
$\nabla f(P)$, we further exploit the problem structure.

\begin{Lem}
Given $A,B$, and $P$, the QAP objective function can be written as
$$ \aligned
f(P)
&= \sum_{i=1}^m \sum_{j=1}^n \left( A \circ PBP^t \right)_{ij}
\label{eq:r23$i$} \\
&= \sum_{i,j=1}^m \left( APB^t \circ P \right)_{ij}
= \sum_{i,j=1}^n \left( A^tPB \circ P \right)_{ij}
\label{eq:r23$ii$} \\
&= \langle s(P), (B \otimes A) s(P) \rangle .
\label{eq:r23$iii$}
\endaligned
$$
Consequently, the gradient
$\nabla f = ( \nabla_Pf  ) \in ( E^{m \times m}, E^{n \times n})$
can be computed as
$$
\aligned
\nabla_Pf(P) &= APB^t+ A^tPB.
\endaligned
\label{eq:r24}
$$

Proof: From Lemma 2.6, $$ \aligned f(P) &= tr\left({ \ APB^t P^t
\ }\right)  \\ &= tr\left({ I_n A I_n (PBP^t)^t
}\right)  \\ &= \langle e_m, (A \circ PBP^t) e_n \rangle = \langle
s(PBP^t), s(A) \rangle \\ &= \langle (P \otimes P)s(B), s(A)
\rangle = \langle (P \otimes I_n) (I_n \otimes P) s(B), s(A)
\rangle \\ &= \langle (I_n \otimes P) s(B), (P^t \otimes I_n) s(A)
\rangle = \langle s(PBI_n), s(I_nAP) \rangle \\ &= \langle s(PB),
s(AP) \rangle \\ &= \langle s(I_nPB), s(API_n) \rangle = \langle
(B^t \otimes I_n) s(P), (I_n \otimes A) s(P) \rangle \\ &= \langle
s(P), (B \otimes I_n) (I_n \otimes A) s(P) \rangle \\ &= \langle
s(P), (B \otimes A) s(P) \rangle = \langle s(P), s(APB^t) \rangle
\\ &= \langle (B^t \otimes A^t) s(P), s(P) \rangle = \langle s(A^t
PB), s(P) \rangle . \qed
\endaligned
$$
\end{Lem}

The Frank-Wolfe method (FW) is an iterative method to approximately solve
a quadratic programming problem.   We note in passing that the Hessian of $f(P)$ is given by $$\nabla^{2}f(P)=B \otimes A+B^{t} \otimes A^{t}$$ 
and that the quadratic programming problem is in general not convex.
FW takes a starting point 
$X^{(0)}\in \mathcal D$ and then proceeds by solving the LAP derived from a first order Taylor series expansion about $X^{(0)}.$   The solution of this LAP defines the
direction to search for the next iterate.  An exact line search is used to find the next iterate,
$X^{(1)}$  which is also a doubly stochastic matrix.  The procedure is iterated until a desired 
accuracy is obtained or convergence.    


\subsubsection{Frank-Wolfe Approach}
In view of the fact the solution to Problem 2.8 is a vertex,
we now consider the Frank-Wolfe algorithm [xx].

%\block \voffset+0.25in \null \hrule \voffset+0.25in \null
\begin{Alg}[Frank-Wolfe Algorithm (FW)]
Given $ X^{(1)} \in \mathcal D_m $.
%\roster \widestnumber\item{xxxxxxxx} \raggedright
\item {Step 0:} Let $k$ = 1.

\item {Step 1:} Compute $W^{(k)}$  by solving the
linear assignment problem:
$$
W^{(k)} = {\text{\rm arg }} \min_{W^{(k)}}
\lbrace { \sum_{i,j=1}^m \left( \nabla_X f(X^{(k)}) \circ W^{(k)} \right)_{ij}
\mid W^{(k)} \in {\mathcal D_m} }\rbrace
$$
\item {Step 2:} Let $d^{(k)} = W^{(k)} - X^{(k)}. $
\item {Step 3:} Find $\alpha^k \in [0,1]$ such that
$$\alpha^{(k)}={\text{\rm arg }} \min_{\alpha^{(k)}}
f(X^{(k)} + \alpha^{(k)} d^{(k)}).
$$
\item {Step 4:} Let

$$ X^{(k+1)} = X^{(k)} + \alpha^{(k)} d_X^{(k)}. $$
\item {Step 5:} 
If $\norm d^{(k)}_{P} \norm \le \epsilon$ or $\alpha^{(k)}=0$ or $k=k_{\max}$
then stop; else $k$ = $k$ + 1 and go to step 1.
\end{Alg}

\begin{Rem}
%\item This implementation is based upon a stop rule that uses the
%norm of the gradient projected onto the doubly stochastic constraint set;
%consequently, any solution produced by the above algorithm satisfies
%the first-order necessary optimality conditions.
%Other stop rules can be used that require less work than the projection and
%will be considered when the general QAP is solved later in the paper.
%\begin{enumerate}
\item At each iteration, $W^{(k)}$ is an extreme point of the
constraint set, a permutation matrix.
However, in general, the iterates $X^{(k)}$ remain in the
interior of the constraint set.
\item Step 3 is a ``line search;'' however, as the objective function is quadratic
the optimum $\alpha$ can be found exactly.
\item Once FW has terminated the nearest permutation to the double stochastic matrix, $X^{(k)}$ can be found as given by the following Lemma.
%\item It is well-known that the algorithm is quadratically
%convergent when in the neighborhood of a KKT point.
%\end{enumerate}
\end{Rem}
%We now consider solving the general form of the quadratic assignment
%problem.
%
%\begin{Def} [General Quadratic Assignment Problem]
%  Given $A,C \in E^{n \times n}$ and $B \in E^{m \times m}$,
%  $$
%  \min_{X \in {\Pi_n}} \langle s(X), (B \otimes A) s(X) \rangle +
%  \langle C, X \rangle \equiv \min_{X \in {\Pi_n}} \langle AXB^t +C, X
%  \rangle .
%  $$
%\end{Def}

\begin{Lem}
  Given $Y \in E^{n \times n}$, the projection of $Y$ onto $\Pi_n$ can
  be obtained by solving the  LAP
  $$
  W={\text{\rm arg }} \min_W \lbrace { \norm W-Y \norm_F \mid W \in
    \Pi_n }\rbrace \equiv {\text{\rm arg }} \max_W \lbrace{ tr(YW^t)
    \mid W \in \mathcal D_n }\rbrace .
  $$
  Proof: Since $W \in \Pi_n \subset \mathcal O$, we have
  $$
  \aligned
  \norm W-Y \norm_F^2 &= tr( (W-Y)^t (W-Y)) \\
% &= {1 \over 2} (\norm Y \norm_F^2 + \norm W \norm_F^2) - tr( Y W^t ) \\
% &= {1 \over 2} (tr(Y^t Y) + tr(W^t W)) - tr( Y W^t ) \\
  &= (\norm Y \norm_F^2 + \norm I \norm_F^2) - 2tr( Y W^t ). \qed
  \endaligned
  $$
\end{Lem}


\subsection{General QAP Test Problems}
%\label{sec:genqap}
%\begin{enumerate}
%\item  QAPLIB - Burkard, Karisch, Rendl [1994] \cite{burkard:1994}
%\item  $n$: 5 - 128
%\end{enumerate}
To illustrate the effectiveness of FW 
we give the performance of the algorithm on 16 sample problems.  The below table gives the optimum value found by FW with 1, 2, 3, and 100 random starting points   and compared 
to the best known solution as given in column 2 and the Path algorithm  \cite{Path:2009}, column 7.   The starting point for FW, $X^{(0)}$ is chosen to be
$$X^{(0)}=\frac {1}{2n} + \frac{1}{2} S$$
where $S$ is a doubly stochastic matrix as computed by running Sinkhorn balancing on a uniform $[0,1]$ matrix.

\begin{table}[htdp]
\caption{Comparison of Frank-Wolfe with Minimum Solution and Path Algorithm}
\begin{center}
\begin{tabular}{|r|r||r|r|r|r|r|}
\hline
Problem  &   Min    & FW$_{100}$&FW$_{3}$&FW$_{2}$&FW$_{1}$&Path\\
\hline
    chr12c &   11156 &   12176 &   13072 &   13072 &   13072 &   18048\\
    chr15a &    9896 &    9896 &   17272 &   17272 &   27584 &   19086\\
    chr15c &    9504 &   10960 &   14274 &   14274 &   17324 &   16206\\
    chr20b &    2298 &    2786 &    3068 &    3068 &    3068 &    5560\\
    chr22b &    6194 &    7218 &    7876 &    7876 &    8482 &    8500\\
    esc16b &     292 &     292 &     294 &     294 &     320 &     300\\
     rou12 &  235528 &  235528 &  238134 &  253684 &  253684 &  256320\\
     rou15 &  354210 &  356654 &  371458 &  371458 &  371458 &  391270\\
     rou20 &  725522 &  730614 &  743884 &  743884 &  743884 &  778284\\
    tai10a &  135028 &  135828 &  148970 &  157954 &  157954 &  152534\\
    tai15a &  388214 &  391522 &  397376 &  397376 &  397376 &  419224\\
    tai17a &  491812 &  496598 &  511574 &  511574 &  529134 &  530978\\
    tai20a &  703482 &  711840 &  721540 &  721540 &  734276 &  753712\\
    tai30a & 1818146 & 1844636 & 1890738 & 1894640 & 1894640 & 1903872\\
    tai35a & 2422002 & 2454292 & 2460940 & 2460940 & 2460940 & 2555110\\
    tai40a & 3139370 & 3187738 & 3194826 & 3194826 & 3227612 & 3281830\\
    \hline
\end{tabular}
\end{center}
\label{tab:fwpath}
\end{table}%

\bibliography{qaprefs}


\end{document}




/* 24 September 1997 */
% ----------------------------------------------------------------------
\Refs %\nofrills{\smc REFERENCES}
\widestnumber\key{77}
\widestnumber\no{77}
